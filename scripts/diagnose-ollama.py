#!/usr/bin/env python3
"""
Diagnostyka i optymalizacja Ollama dla test√≥w Playwright
"""

import requests
import json
import sys
import psutil

def check_ollama_status():
    """Sprawd≈∫ czy Ollama jest uruchomiona"""
    print("üîç Sprawdzanie statusu Ollama...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama jest uruchomiona")
            models = response.json().get('models', [])
            if models:
                print(f"\nüìã Dostƒôpne modele ({len(models)}):")
                for model in models:
                    print(f"  - {model.get('name', 'unknown')}")
                return True, models
            else:
                print("‚ö†Ô∏è  Ollama dzia≈Ça, ale brak zainstalowanych modeli")
                return True, []
        else:
            print(f"‚ùå Ollama odpowiada z kodem: {response.status_code}")
            return False, []
    except requests.exceptions.ConnectionError:
        print("‚ùå Nie mo≈ºna po≈ÇƒÖczyƒá siƒô z Ollama (http://localhost:11434)")
        print("   Uruchom: ollama serve")
        return False, []
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd: {e}")
        return False, []

def check_system_resources():
    """Sprawd≈∫ dostƒôpne zasoby systemowe"""
    print("\nüíæ Sprawdzanie zasob√≥w systemowych...")
    
    # RAM
    ram = psutil.virtual_memory()
    ram_total_gb = ram.total / (1024**3)
    ram_available_gb = ram.available / (1024**3)
    
    print(f"  RAM: {ram_available_gb:.1f}GB dostƒôpne / {ram_total_gb:.1f}GB ≈ÇƒÖcznie")
    
    # CPU
    cpu_count = psutil.cpu_count()
    print(f"  CPU: {cpu_count} rdzeni")
    
    return ram_total_gb, ram_available_gb

def suggest_model(ram_total_gb):
    """Zasugeruj model na podstawie dostƒôpnej pamiƒôci"""
    print("\nü§ñ Rekomendacje modelu:")
    
    if ram_total_gb >= 16:
        print("  ‚úÖ Zalecany model: llama3.1:8b lub llama3.2")
        print("     Instalacja: ollama pull llama3.1:8b")
        return "llama3.1:8b"
    elif ram_total_gb >= 8:
        print("  ‚ö†Ô∏è  Zalecany model: llama3.2:3b lub qwen2.5:3b")
        print("     Instalacja: ollama pull llama3.2:3b")
        return "llama3.2:3b"
    else:
        print("  ‚ö†Ô∏è  Zalecany model: phi3:mini lub tinyllama")
        print("     Instalacja: ollama pull phi3:mini")
        return "phi3:mini"

def configure_model(model_name):
    """Skonfiguruj parametry modelu dla stabilno≈õci test√≥w"""
    print(f"\n‚öôÔ∏è  Konfiguracja modelu {model_name}...")
    
    modelfile_content = f"""FROM {model_name}

# Parametry dla stabilno≈õci test√≥w
PARAMETER temperature 0
PARAMETER num_ctx 8192
PARAMETER top_k 10
PARAMETER top_p 0.9

SYSTEM You are a precise web automation assistant. Your task is to identify UI elements on web pages with high accuracy. Always provide specific, unambiguous selectors.
"""
    
    print("\nüìù Zapisz poni≈ºszƒÖ konfiguracjƒô do pliku 'Modelfile':")
    print("=" * 60)
    print(modelfile_content)
    print("=" * 60)
    print("\nNastƒôpnie uruchom:")
    print(f"  ollama create playwright-{model_name} -f Modelfile")
    
    return modelfile_content

def main():
    print("=" * 60)
    print("üöÄ Diagnostyka Ollama dla Playwright AI Tests")
    print("=" * 60)
    
    # Krok 1: Sprawd≈∫ status Ollama
    is_running, models = check_ollama_status()
    
    if not is_running:
        print("\n‚ùå Ollama nie dzia≈Ça. Uruchom jƒÖ przed kontynuowaniem.")
        sys.exit(1)
    
    # Krok 2: Sprawd≈∫ zasoby systemowe
    ram_total, ram_available = check_system_resources()
    
    # Krok 3: Zasugeruj model
    suggested_model = suggest_model(ram_total)
    
    # Krok 4: Sprawd≈∫ czy sugerowany model jest zainstalowany
    model_names = [m.get('name', '') for m in models]
    if any(suggested_model in name for name in model_names):
        print(f"\n‚úÖ Model {suggested_model} jest ju≈º zainstalowany")
    else:
        print(f"\n‚ö†Ô∏è  Model {suggested_model} nie jest zainstalowany")
        print(f"   Uruchom: ollama pull {suggested_model}")
    
    # Krok 5: Wygeneruj konfiguracjƒô
    modelfile = configure_model(suggested_model)
    
    # Zapisz Modelfile
    with open('Modelfile', 'w') as f:
        f.write(modelfile)
    print("\n‚úÖ Plik 'Modelfile' zosta≈Ç utworzony")
    
    print("\n" + "=" * 60)
    print("‚úÖ Diagnostyka zako≈Ñczona pomy≈õlnie!")
    print("=" * 60)
    
    print("\nüìã Nastƒôpne kroki:")
    print("  1. ollama create playwright-model -f Modelfile")
    print("  2. Zaktualizuj .env z nowym modelem")
    print("  3. npm test")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Przerwano przez u≈ºytkownika")
        sys.exit(0)
