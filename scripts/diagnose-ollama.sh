#!/bin/bash
# Skrypt diagnostyczny dla Ollama

echo "=========================================="
echo "ðŸ” Diagnostyka Ollama"
echo "=========================================="

# SprawdÅº czy Ollama jest uruchomiona
echo ""
echo "Sprawdzanie statusu Ollama..."
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Ollama jest uruchomiona"
    
    echo ""
    echo "ðŸ“‹ DostÄ™pne modele:"
    curl -s http://localhost:11434/api/tags | python3 -m json.tool | grep '"name"' | cut -d'"' -f4
else
    echo "âŒ Ollama nie jest uruchomiona"
    echo "   Uruchom: ollama serve"
    exit 1
fi

# SprawdÅº zasoby systemowe
echo ""
echo "ðŸ’¾ Zasoby systemowe:"
if command -v free &> /dev/null; then
    free -h | grep Mem | awk '{print "  RAM: " $3 " uÅ¼ywane / " $2 " Å‚Ä…cznie"}'
elif command -v vm_stat &> /dev/null; then
    # macOS
    echo "  SprawdÅº pamiÄ™Ä‡: Activity Monitor"
else
    echo "  Nie moÅ¼na sprawdziÄ‡ pamiÄ™ci"
fi

echo ""
echo "ðŸ¤– Zalecany model: llama3.1:8b"
echo "   Instalacja: ollama pull llama3.1:8b"

echo ""
echo "=========================================="
echo "âœ… Diagnostyka zakoÅ„czona"
echo "=========================================="
