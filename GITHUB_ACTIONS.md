# GitHub Actions z Ollama - Przewodnik

## üìã Spis tre≈õci

- [üéØ Mo≈ºliwo≈õci u≈ºycia Ollama w GitHub Actions](#-mo≈ºliwo≈õci-u≈ºycia-ollama-w-github-actions)
  - [‚úÖ Opcja 1: Self-Hosted Runner (Zalecane)](#-opcja-1-self-hosted-runner-zalecane)
  - [‚ö° Opcja 2: GitHub-Hosted Runner z ma≈Çym modelem](#-opcja-2-github-hosted-runner-z-ma≈Çym-modelem)
  - [üåê Opcja 3: Zewnƒôtrzny serwis Ollama + Tunel](#-opcja-3-zewnƒôtrzny-serwis-ollama--tunel)
  - [üí° Opcja 4: OpenAI API dla CI (Fallback)](#-opcja-4-openai-api-dla-ci-fallback)
- [üìä Por√≥wnanie opcji](#-por√≥wnanie-opcji)
- [üöÄ Najlepsza praktyka](#-najlepsza-praktyka)
- [üìù Workflow Files](#-workflow-files)
- [üîß Troubleshooting](#-troubleshooting)

---

## üéØ Mo≈ºliwo≈õci u≈ºycia Ollama w GitHub Actions

### ‚úÖ Opcja 1: Self-Hosted Runner (Zalecane)

**Zalety:**

- Pe≈Çna kontrola nad zasobami
- Mo≈ºliwo≈õƒá u≈ºycia GPU
- Szybkie dzia≈Çanie z du≈ºymi modelami
- Brak limit√≥w czasowych GitHub
- Najlepsza wydajno≈õƒá

**Kroki konfiguracji:**

1. **Zainstaluj GitHub Actions Runner na swoim komputerze:**

   ```bash
   # Pobierz runner
   mkdir actions-runner && cd actions-runner

   # Linux/macOS
   curl -o actions-runner-linux-x64-2.311.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.311.0/actions-runner-linux-x64-2.311.0.tar.gz
   tar xzf ./actions-runner-linux-x64-2.311.0.tar.gz

   # Windows PowerShell
   Invoke-WebRequest -Uri https://github.com/actions/runner/releases/download/v2.311.0/actions-runner-win-x64-2.311.0.zip -OutFile actions-runner-win-x64-2.311.0.zip
   Expand-Archive -Path ./actions-runner-win-x64-2.311.0.zip -DestinationPath .
   ```

2. **Konfiguruj runner:**

   ```bash
   # Przejd≈∫ do Settings ‚Üí Actions ‚Üí Runners ‚Üí New self-hosted runner
   # Skopiuj token i uruchom:
   ./config.sh --url https://github.com/TWOJE_KONTO/REPO --token TWOJ_TOKEN

   # Uruchom runner
   ./run.sh
   ```

3. **Upewnij siƒô ≈ºe Ollama dzia≈Ça:**

   ```bash
   ollama serve  # W osobnym terminalu
   ollama pull llama3.2-vision:latest
   ```

4. **U≈ºyj workflow:** `.github/workflows/self-hosted-tests.yml`

---

### ‚ö° Opcja 2: GitHub-Hosted Runner z ma≈Çym modelem

**Zalety:**

- Nie wymaga konfiguracji runnera
- Bezp≈Çatne dla publicznych repozytori√≥w
- Automatyczne zarzƒÖdzanie

**Wady:**

- Ograniczone zasoby (2 CPU, 7GB RAM)
- Brak GPU (wolne dzia≈Çanie CPU)
- Tylko ma≈Çe modele (phi3:mini, tinyllama)
- Timeout 6h dla job

**Kroki:**

1. **U≈ºyj workflow:** `.github/workflows/playwright-tests.yml`

2. **Dostosuj timeout w playwright.config.ts dla CI:**

   ```typescript
   timeout: process.env.CI ? 180000 : 90000,  // 3 min dla CI
   ```

3. **U≈ºyj ma≈Çego modelu w .env CI:**

   ```bash
   OLLAMA_MODEL=phi3:mini  # ~2.3GB, szybszy ni≈º llama3
   ```

**Ograniczenia:**

- Testy mogƒÖ byƒá 3-5x wolniejsze ni≈º lokalnie
- U≈ºyj tylko tag√≥w `@smoke` dla CI
- Rozwa≈º zmniejszenie timeout√≥w

---

### üåê Opcja 3: Zewnƒôtrzny serwis Ollama + Tunel

**Scenariusz:** Ollama dzia≈Ça na twoim serwerze/komputerze, GitHub Actions ≈ÇƒÖczy siƒô przez tunel.

**Kroki:**

1. **Uruchom Ollama z ekspozycjƒÖ na zewnƒÖtrz:**

   ```bash
   # Linux/macOS
   OLLAMA_HOST=0.0.0.0:11434 ollama serve

   # Windows PowerShell
   $env:OLLAMA_HOST="0.0.0.0:11434"
   ollama serve
   ```

2. **Ustaw tunel (ngrok lub cloudflare tunnel):**

   ```bash
   # ngrok
   ngrok http 11434

   # lub cloudflare tunnel
   cloudflared tunnel --url http://localhost:11434
   ```

3. **Dodaj URL tunelu do GitHub Secrets:**
   - Settings ‚Üí Secrets ‚Üí Actions ‚Üí New repository secret
   - Nazwa: `OLLAMA_BASE_URL`
   - Warto≈õƒá: `https://your-tunnel-url.ngrok.io/v1`

4. **Zaktualizuj workflow:**

   ```yaml
   - name: Update .env for CI
     run: |
       echo "OLLAMA_BASE_URL=${{ secrets.OLLAMA_BASE_URL }}" > .env
       echo "OLLAMA_API_KEY=ollama" >> .env
       echo "OLLAMA_MODEL=llama3.2-vision:latest" >> .env
   ```

**Uwaga:** Bezpiecze≈Ñstwo! U≈ºywaj tylko w prywatnych repozytoriach lub dodaj autentykacjƒô.

---

### üí° Opcja 4: OpenAI API dla CI (Fallback)

Je≈õli Ollama jest zbyt wolna dla CI, u≈ºyj OpenAI API tylko w ≈õrodowisku CI:

1. **Dodaj OpenAI API key do GitHub Secrets:**
   - `OPENAI_API_KEY`

2. **Zaktualizuj AIHelper aby obs≈Çugiwa≈Ç oba API:**

   ```typescript
   constructor(page: Page) {
     this.page = page;

     const isCI = process.env.CI === 'true';
     const baseURL = isCI
       ? 'https://api.openai.com/v1'  // OpenAI dla CI
       : process.env.OLLAMA_BASE_URL;  // Ollama lokalnie

     const apiKey = isCI
       ? process.env.OPENAI_API_KEY
       : process.env.OLLAMA_API_KEY;

     this.client = new OpenAI({ baseURL, apiKey });
   }
   ```

3. **Workflow z OpenAI:**

   ```yaml
   - name: Update .env for CI
     run: |
       echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" > .env
       echo "CI=true" >> .env
   ```

---

## üìä Por√≥wnanie opcji

| Opcja                      | Szybko≈õƒá   | Koszty      | Z≈Ço≈ºono≈õƒá | Zalecane dla  |
| -------------------------- | ---------- | ----------- | --------- | ------------- |
| Self-hosted                | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Bezp≈Çatne   | ‚≠ê‚≠ê‚≠ê    | Produkcja     |
| GitHub-hosted + ma≈Çy model | ‚≠ê‚≠ê       | Bezp≈Çatne\* | ‚≠ê        | Smoke tests   |
| Tunel                      | ‚≠ê‚≠ê‚≠ê‚≠ê   | Bezp≈Çatne   | ‚≠ê‚≠ê‚≠ê‚≠ê  | Tymczasowe    |
| OpenAI API                 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~$0.01/test | ‚≠ê        | CI/CD szybkie |

\*Bezp≈Çatne dla publicznych repo, 2000 min/miesiƒÖc dla prywatnych

---

## üöÄ Najlepsza praktyka

**Rekomendacja dla projektu:**

1. **Lokalnie:** Ollama z llama3.2-vision:latest
2. **CI/CD:** Self-hosted runner z Ollama (je≈õli mo≈ºliwe)
3. **Backup CI:** GitHub-hosted + phi3:mini tylko dla `@smoke` test√≥w
4. **Produkcja:** OpenAI API (szybsze, bardziej stabilne)

---

## üìù Workflow Files

Projekt zawiera 2 gotowe workflows:

- `.github/workflows/playwright-tests.yml` - GitHub-hosted runner
- `.github/workflows/self-hosted-tests.yml` - Self-hosted runner

Wybierz odpowiedni dla swoich potrzeb lub u≈ºyj obu!

---

## üîß Troubleshooting

### Problem: Ollama nie startuje w CI

```bash
# Dodaj wiƒôcej czasu na uruchomienie
sleep 10  # zamiast sleep 5
```

### Problem: Model za du≈ºy

```bash
# U≈ºyj mniejszego modelu
ollama pull tinyllama  # tylko 637MB
```

### Problem: Timeout test√≥w

```bash
# Zwiƒôksz timeouty w playwright.config.ts
timeout: 180000,  // 3 minuty
```

### Problem: Brak pamiƒôci

```bash
# U≈ºyj self-hosted runner z wiƒôcej RAM
# lub zmniejsz num_ctx
OLLAMA_NUM_CTX=4096  # zamiast 8192
```
