# GitHub Actions z Ollama - Przewodnik

## ğŸ“‹ Spis treÅ›ci

- [ğŸ¯ MoÅ¼liwoÅ›ci uÅ¼ycia Ollama w GitHub Actions](#-moÅ¼liwoÅ›ci-uÅ¼ycia-ollama-w-github-actions)
  - [âœ… Opcja 1: Self-Hosted Runner (Zalecane)](#-opcja-1-self-hosted-runner-zalecane)
  - [âš¡ Opcja 2: GitHub-Hosted Runner z maÅ‚ym modelem](#-opcja-2-github-hosted-runner-z-maÅ‚ym-modelem)
  - [ğŸŒ Opcja 3: ZewnÄ™trzny serwis Ollama + Tunel](#-opcja-3-zewnÄ™trzny-serwis-ollama--tunel)
  - [ğŸ’¡ Opcja 4: OpenAI API dla CI (Fallback)](#-opcja-4-openai-api-dla-ci-fallback)
- [ğŸ“Š PorÃ³wnanie opcji](#-porÃ³wnanie-opcji)
- [ğŸš€ Najlepsza praktyka](#-najlepsza-praktyka)
- [ğŸ“ Workflow Files](#-workflow-files)
- [ğŸ”§ Troubleshooting](#-troubleshooting)

---

## ğŸ¯ MoÅ¼liwoÅ›ci uÅ¼ycia Ollama w GitHub Actions

### âœ… Opcja 1: Self-Hosted Runner (Zalecane)

**Zalety:**

- PeÅ‚na kontrola nad zasobami
- MoÅ¼liwoÅ›Ä‡ uÅ¼ycia GPU
- Szybkie dziaÅ‚anie z duÅ¼ymi modelami
- Brak limitÃ³w czasowych GitHub
- Najlepsza wydajnoÅ›Ä‡

**Kroki konfiguracji:**

1. **Zainstaluj GitHub Actions Runner na swoim komputerze:**

   **Linux/macOS:**

   ```bash
   mkdir actions-runner && cd actions-runner
   curl -o actions-runner-linux-x64-2.321.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.321.0/actions-runner-linux-x64-2.321.0.tar.gz
   tar xzf ./actions-runner-linux-x64-2.321.0.tar.gz
   ```

   **Windows PowerShell (Uruchom jako Administrator dla instalacji jako serwis):**

   ```powershell
   # UtwÃ³rz katalog dla runnera (najlepiej C:\actions-runner)
   mkdir C:\actions-runner
   cd C:\actions-runner

   # Pobierz runner
   Invoke-WebRequest -Uri https://github.com/actions/runner/releases/download/v2.321.0/actions-runner-win-x64-2.321.0.zip -OutFile actions-runner-win-x64-2.321.0.zip

   # Rozpakuj
   Expand-Archive -Path actions-runner-win-x64-2.321.0.zip -DestinationPath . -Force
   ```

2. **Wygeneruj token rejestracyjny:**

   **Opcja A: Przez GitHub CLI (zalecane, automatyczne):**

   ```bash
   gh api /repos/OWNER/REPO/actions/runners/registration-token -X POST | jq -r .token
   ```

   **Opcja B: RÄ™cznie przez UI:**
   - PrzejdÅº do: `Settings â†’ Actions â†’ Runners â†’ New self-hosted runner`
   - Skopiuj token (zaczyna siÄ™ od `AAAA...`)

3. **Konfiguruj runner:**

   **Linux/macOS:**

   ```bash
   ./config.sh --url https://github.com/OWNER/REPO --token TWOJ_TOKEN --name "local-runner"
   ```

   **Windows:**

   ```powershell
   .\config.cmd --url https://github.com/OWNER/REPO --token TWOJ_TOKEN --name "local-windows-runner"
   ```

   **Opcje przy konfiguracji:**
   - Runner group: naciÅ›nij Enter (Default)
   - Run as service: naciÅ›nij `N` (rÄ™czna instalacja, pÃ³Åºniej zainstalujemy jako serwis)

4. **Uruchom runner jako serwis Windows (ZALECANE):**

   **âš ï¸ WAÅ»NE dla Windows:** `svc.cmd` nie istnieje w najnowszych wersjach runnera. UÅ¼yj poniÅ¼szego skryptu:

   **OtwÃ³rz PowerShell jako Administrator** i wykonaj:

   ```powershell
   # PrzejdÅº do katalogu runnera
   cd C:\actions-runner

   # Zainstaluj jako serwis Windows
   New-Service -Name "actions.runner.leonardust-playwright-agents.local-windows-runner" `
       -BinaryPathName "C:\actions-runner\bin\Runner.Listener.exe run" `
       -DisplayName "GitHub Actions Runner (playwright-agents)" `
       -Description "GitHub Actions self-hosted runner" `
       -StartupType Automatic

   # Uruchom serwis
   Start-Service "actions.runner.leonardust-playwright-agents.local-windows-runner"

   # SprawdÅº status
   Get-Service "actions.runner.leonardust-playwright-agents.local-windows-runner"
   ```

   **KorzyÅ›ci instalacji jako serwis:**
   - âœ… Automatyczny start po restarcie komputera
   - âœ… Automatyczny restart po aktualizacji runnera
   - âœ… Nie musisz trzymaÄ‡ terminala otwartego
   - âœ… Job wykona siÄ™ automatycznie po pushu na GitHub

   **Alternatywnie - Linux/macOS:**

   ```bash
   sudo ./svc.sh install
   sudo ./svc.sh start
   ```

   **Alternatywnie - uruchomienie rÄ™czne (musisz zostawiÄ‡ okno otwarte):**

   Linux/macOS:

   ```bash
   ./run.sh
   ```

   Windows:

   ```powershell
   .\run.cmd
   ```

5. **Upewnij siÄ™ Å¼e Ollama dziaÅ‚a:**

   ```bash
   ollama serve  # W osobnym terminalu (jeÅ›li nie dziaÅ‚a jako serwis)
   ollama pull llama3.2-vision:latest
   ```

6. **UÅ¼yj workflow:** `.github/workflows/self-hosted-tests.yml`

**ğŸ’¡ Troubleshooting:**

- **Runner siÄ™ zatrzymuje po aktualizacji (tylko dla rÄ™cznego uruchamiania):** JeÅ›li runner NIE jest zainstalowany jako serwis, musisz uruchomiÄ‡ ponownie `./run.sh` lub `.\run.cmd` po aktualizacji. **RozwiÄ…zanie:** Zainstaluj jako serwis (krok 4)
- **"Waiting for a runner" w GitHub Actions:**
  - SprawdÅº czy runner dziaÅ‚a:
    - Windows: `Get-Service "actions.runner.*"` lub `Get-Process | Where-Object {$_.ProcessName -like "*Runner*"}`
    - Linux: `ps aux | grep Runner`
  - JeÅ›li nie dziaÅ‚a, uruchom serwis: `Start-Service "actions.runner.*"` (Windows) lub `sudo ./svc.sh start` (Linux)
- **"A session for this runner already exists":** Poprzednia sesja wisi. UsuÅ„ runnera i skonfiguruj ponownie:
  ```powershell
  # UsuÅ„ stary runner
  .\config.cmd remove --token <REMOVE_TOKEN>
  # Skonfiguruj od nowa (krok 3)
  ```
- **Brak uprawnieÅ„ na Windows:** Uruchom PowerShell jako Administrator dla instalacji jako serwis
- **Serwis nie startuje:** SprawdÅº logi w `C:\actions-runner\_diag\` lub Event Viewer (Windows Logs â†’ Application)

---

### âš¡ Opcja 2: GitHub-Hosted Runner z maÅ‚ym modelem

**Zalety:**

- Nie wymaga konfiguracji runnera
- BezpÅ‚atne dla publicznych repozytoriÃ³w
- Automatyczne zarzÄ…dzanie

**Wady:**

- Ograniczone zasoby (2 CPU, 7GB RAM)
- Brak GPU (wolne dziaÅ‚anie CPU)
- Tylko maÅ‚e modele (phi3:mini, tinyllama)
- Timeout 6h dla job

**Kroki:**

1. **UÅ¼yj workflow:** `.github/workflows/playwright-tests.yml`

2. **Dostosuj timeout w playwright.config.ts dla CI:**

   ```typescript
   timeout: process.env.CI ? 180000 : 90000,  // 3 min dla CI
   ```

3. **UÅ¼yj maÅ‚ego modelu w .env CI:**

   ```bash
   OLLAMA_MODEL=phi3:mini  # ~2.3GB, szybszy niÅ¼ llama3
   ```

**Ograniczenia:**

- Testy mogÄ… byÄ‡ 3-5x wolniejsze niÅ¼ lokalnie
- UÅ¼yj tylko tagÃ³w `@smoke` dla CI
- RozwaÅ¼ zmniejszenie timeoutÃ³w

---

### ğŸŒ Opcja 3: ZewnÄ™trzny serwis Ollama + Tunel

**Scenariusz:** Ollama dziaÅ‚a na twoim serwerze/komputerze, GitHub Actions Å‚Ä…czy siÄ™ przez tunel.

**Kroki:**

1. **Uruchom Ollama z ekspozycjÄ… na zewnÄ…trz:**

   ```bash
   # Linux/macOS
   OLLAMA_HOST=0.0.0.0:11434 ollama serve

   # Windows PowerShell
   $env:OLLAMA_HOST="0.0.0.0:11434"
   ollama serve
   ```

2. **Ustaw tunel (ngrok lub cloudflare tunnel):**

   ```bash
   # ngrok
   ngrok http 11434

   # lub cloudflare tunnel
   cloudflared tunnel --url http://localhost:11434
   ```

3. **Dodaj URL tunelu do GitHub Secrets:**
   - Settings â†’ Secrets â†’ Actions â†’ New repository secret
   - Nazwa: `OLLAMA_BASE_URL`
   - WartoÅ›Ä‡: `https://your-tunnel-url.ngrok.io/v1`

4. **Zaktualizuj workflow:**

   ```yaml
   - name: Update .env for CI
     run: |
       echo "OLLAMA_BASE_URL=${{ secrets.OLLAMA_BASE_URL }}" > .env
       echo "OLLAMA_API_KEY=ollama" >> .env
       echo "OLLAMA_MODEL=llama3.2-vision:latest" >> .env
   ```

**Uwaga:** BezpieczeÅ„stwo! UÅ¼ywaj tylko w prywatnych repozytoriach lub dodaj autentykacjÄ™.

---

### ğŸ’¡ Opcja 4: OpenAI API dla CI (Fallback)

JeÅ›li Ollama jest zbyt wolna dla CI, uÅ¼yj OpenAI API tylko w Å›rodowisku CI:

1. **Dodaj OpenAI API key do GitHub Secrets:**
   - `OPENAI_API_KEY`

2. **Zaktualizuj AIHelper aby obsÅ‚ugiwaÅ‚ oba API:**

   ```typescript
   constructor(page: Page) {
     this.page = page;

     const isCI = process.env.CI === 'true';
     const baseURL = isCI
       ? 'https://api.openai.com/v1'  // OpenAI dla CI
       : process.env.OLLAMA_BASE_URL;  // Ollama lokalnie

     const apiKey = isCI
       ? process.env.OPENAI_API_KEY
       : process.env.OLLAMA_API_KEY;

     this.client = new OpenAI({ baseURL, apiKey });
   }
   ```

3. **Workflow z OpenAI:**

   ```yaml
   - name: Update .env for CI
     run: |
       echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" > .env
       echo "CI=true" >> .env
   ```

---

## ğŸ“Š PorÃ³wnanie opcji

| Opcja                      | SzybkoÅ›Ä‡   | Koszty      | ZÅ‚oÅ¼onoÅ›Ä‡ | Zalecane dla  |
| -------------------------- | ---------- | ----------- | --------- | ------------- |
| Self-hosted                | â­â­â­â­â­ | BezpÅ‚atne   | â­â­â­    | Produkcja     |
| GitHub-hosted + maÅ‚y model | â­â­       | BezpÅ‚atne\* | â­        | Smoke tests   |
| Tunel                      | â­â­â­â­   | BezpÅ‚atne   | â­â­â­â­  | Tymczasowe    |
| OpenAI API                 | â­â­â­â­â­ | ~$0.01/test | â­        | CI/CD szybkie |

\*BezpÅ‚atne dla publicznych repo, 2000 min/miesiÄ…c dla prywatnych

---

## ğŸš€ Najlepsza praktyka

**Rekomendacja dla projektu:**

1. **Lokalnie:** Ollama z llama3.2-vision:latest
2. **CI/CD:** Self-hosted runner z Ollama (jeÅ›li moÅ¼liwe)
3. **Backup CI:** GitHub-hosted + phi3:mini tylko dla `@smoke` testÃ³w
4. **Produkcja:** OpenAI API (szybsze, bardziej stabilne)

---

## ğŸ“ Workflow Files

Projekt zawiera 2 gotowe workflows:

- `.github/workflows/playwright-tests.yml` - GitHub-hosted runner
- `.github/workflows/self-hosted-tests.yml` - Self-hosted runner

Wybierz odpowiedni dla swoich potrzeb lub uÅ¼yj obu!

---

## ğŸ”§ Troubleshooting

### Problem: Ollama nie startuje w CI

```bash
# Dodaj wiÄ™cej czasu na uruchomienie
sleep 10  # zamiast sleep 5
```

### Problem: Model za duÅ¼y

```bash
# UÅ¼yj mniejszego modelu
ollama pull tinyllama  # tylko 637MB
```

### Problem: Timeout testÃ³w

```bash
# ZwiÄ™ksz timeouty w playwright.config.ts
timeout: 180000,  // 3 minuty
```

### Problem: Brak pamiÄ™ci

```bash
# UÅ¼yj self-hosted runner z wiÄ™cej RAM
# lub zmniejsz num_ctx
OLLAMA_NUM_CTX=4096  # zamiast 8192
```
